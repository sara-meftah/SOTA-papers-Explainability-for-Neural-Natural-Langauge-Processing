# Tracking SOTA papers about Explainability for Neural Natural Langauge Processing


#### [Surveys](#content)
1. **Analysis methods in neural language processing: A survey.** - 2020 - Transactions of the Association for Computational Linguistics [paper](https://aclanthology.org/Q19-1004.pdf)
2. **A Survey on Explainability in Machine Reading Comprehension.** - 2020 - Arxiv [paper](https://arxiv.org/abs/2010.00389)
3. **A Diagnostic Study of Explainability Techniques for Text Classification.** - 2020 - Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/2020.emnlp-main.263.pdf) 
4. **A Survey of the State of Explainable AI for Natural Language Processing.** - 2020 - AACL 2020 [paper](https://arxiv.org/abs/2010.00711) [site](https://xainlp2020.github.io/xainlp/)
5. **( Not NLP) Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics** - 2021 - MDPI [paper](https://www.mdpi.com/2079-9292/10/5/593)  



#### [Evaluation](#content)

1. **( Not NLP) Evaluating the visualization of what a Deep Neural Network has learned** - 2016 - IEEE transactions on neural networks and learning systems [paper](https://arxiv.org/pdf/1509.06321.pdf)
2.  **( Not NLP) Interpretation of Neural Networks is Fragile** - 2017 - 31st Conference on Neural Information Processing Systems (NIPS 2017) [paper](https://machine-learning-and-security.github.io/papers/mlsec17_paper_18.pdf)
3. **( Not NLP) On the Robustness of Interpretability Methods** - 2018 - Arxiv [paper](https://arxiv.org/abs/1806.08049)
4. **Comparing automatic and human evaluation of local explanations for text classification** - 2018 - Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics [paper](https://aclanthology.org/N18-1097.pdf)
5. **( Not NLP) On the (In)fidelity and Sensitivity of Explanations** - 2019 - 33rd Conference on Neural Information Processing Systems (NeurIPS 2019) [paper](https://papers.nips.cc/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf)  [code](https://github.com/chihkuanyeh/saliency_evaluation)
6. **( Not NLP) A Benchmark for Interpretability Methods in Deep Neural Networks** - 2019 - 33rd Conference on Neural Information Processing Systems (NeurIPS 2019) [paper](https://proceedings.neurips.cc/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf) 
7. **Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?** - 2020 - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/2020.acl-main.491.pdf) [code](https://github.com/peterbhase/InterpretableNLP-ACL2020)
8. **Evaluating Explanation Methods for Neural Machine Translation** - 2020 - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/2020.acl-main.35.pdf)
9. **Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?** - 2020 - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/2020.acl-main.386.pdf)
10. **ERASER: A Benchmark to Evaluate Rationalized NLP Models** - 2020 - Transactions of the Association for Computational Linguistics [paper](https://aclanthology.org/2020.acl-main.408.pdf)  [code](https://github.com/jayded/eraserbenchmark)  [site](http://www.eraserbenchmark.com/)
11. **( Not NLP) Fooling lime and shap: Adversarial attacks on post hoc explanation methods** - 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society [paper](https://dl.acm.org/doi/pdf/10.1145/3375627.3375830) [official code](https://github.com/dylan-slack/Fooling-LIME-SHAP)
12. **( Not NLP) What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors** - 2021 - KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining [paper](https://dl.acm.org/doi/10.1145/3447548.3467213)  
13. **The Eval4NLP Shared Task on Explainable Quality Estimation: Overview and Results.** - 2021 - Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems [paper](https://aclanthology.org/2021.eval4nlp-1.17.pdf)
14. **Measuring and Improving Faithfulness of Attention in Neural Machine Translation** - 2021 - Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics [paper](https://aclanthology.org/2021.eacl-main.243.pdf)


#### [Datasets](#content)

1. **HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.** - 2018 - Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D18-1259/) [data](https://hotpotqa.github.io/)
2. **GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING.** - 2019 - Arxiv [paper](https://openreview.net/pdf?id=rJ4km2R5t7) [code](https://github.com/axa-rev-research/quackie)  [site](https://gluebenchmark.com/tasks)
3. **QUACKIE: A NLP Classification Task With Ground Truth Explanations.** - 2020 - Arxiv [paper](https://aclanthology.org/D18-1259/) [code](https://github.com/axa-rev-research/quackie)  [site](https://axa-rev-research.github.io/quackie/)
4. **ERASER: A Benchmark to Evaluate Rationalized NLP Models** - 2020 - Transactions of the Association for Computational Linguistics [paper](https://aclanthology.org/2020.acl-main.408.pdf)  [code](https://github.com/jayded/eraserbenchmark)  [site](http://www.eraserbenchmark.com/)



#### [Probing Classifiers](#content)

1. **Does string-based neural MT learn source syntax?** - 2016 - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D16-1159.pdf)
2. **Fine-grained analysis of sentence embeddings using auxiliary prediction tasks.** - 2016 - Proceedings of ICLR Conference Track [paper](https://arxiv.org/pdf/1608.04207.pdf)
3. **Assessing the ability of LSTMs to learn syntax-sensitive dependencies.** - 2016 - Transactions of the Association for Computational Linguistics [paper](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00115/43378/Assessing-the-Ability-of-LSTMs-to-Learn-Syntax) [code](https://github.com/TalLinzen/rnn_agreement)
4. **What you can cram into a single vector: Probing sentence embeddings for linguistic properties.** - 2018 - Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P18-1198.pdf) [data](https://github.com/facebookresearch/SentEval/tree/main/data/probing)
5. **Syntactic structure from deep learning.** - 2021 - Annual Review of Linguistics [paper](https://www.annualreviews.org/doi/abs/10.1146/annurev-linguistics-032020-051035) 
6. **Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?** - 2021 - Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics [paper](https://aclanthology.org/2021.eacl-main.295.pdf) 
7. **Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals.** - 2021 - Transactions of the Association for Computational Linguistics [paper](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00359/98091)  [code](https://github.com/yanaiela/amnesic_probing) 


#### [Surrogate models](#content)

1. **Why should i trust you?: Explaining the predictions of any classifier.** - 2016 - Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining [paper](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf) [code](https://github.com/marcotcr/lime) 
2. **A causal framework for explaining the predictions of black-box sequence-to-sequence models.** - 2017 - Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D17-1042.pdf) 
3. **( Not NLP) "Why Should You Trust My Explanation?" Understanding Uncertainty in LIME Explanations** - 2019 - Appearing at the International Conference on Machine Learning AI for Social Good Workshop [paper](https://arxiv.org/abs/1904.12991)
4.  **An analysis of lime for text data.** - 2021 - Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS) 2021 [paper](http://proceedings.mlr.press/v130/mardaoui21a/mardaoui21a.pdf) 
5. **Explaining the explainer: A first theoretical analysis of LIME.** - 2020 - Proceedings of the 23th International Conference on Artificial Intelligence and Statistics (AISTATS) 2020 [paper](http://proceedings.mlr.press/v108/garreau20a/garreau20a.pdf) 
6. **( Not NLP) Fooling lime and shap: Adversarial attacks on post hoc explanation methods** - 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society [paper](https://dl.acm.org/doi/pdf/10.1145/3375627.3375830) [official code](https://github.com/dylan-slack/Fooling-LIME-SHAP)
7. **Deterministic Local Interpretable Model-Agnostic Explanations for Stable Explainability.** - 2021 - Special Issue Advances in Explainable Artificial Intelligence (XAI) [paper](https://aclanthology.org/D18-1259/) [code](https://www.mdpi.com/2504-4990/3/3/27/htm)  [site](https://github.com/rehmanzafar/dlime_experiments)
8. **Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing.** - 2021 - Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP [paper](https://aclanthology.org/2021.blackboxnlp-1.33.pdf) [code](https://github.com/qdata/textattack-fragile-interpretations)

#### [Supervised Selective Rationalization](#content)

1. **Using “annotator rationales” to improve machine learning for text categorization** - 2007 - Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics (NAACL 2007) [paper](https://aclanthology.org/N07-1033/) 
2. **Rationale-Augmented Convolutional Neural Networks for Text Classification** - 2016 - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D16-1076/)  [official code](https://github.com/bwallace/rationale-CNN)  [other implementation](https://github.com/yezhang-xiaofan/Rationale-CNN) 
3. **Training Classifiers with Natural Language Explanations** - 2018 - Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P18-1175.pdf) 
4. **e-SNLI: Natural Language Inference with Natural Language Explanations** - 2018 - 32nd Conference on Neural Information Processing Systems (NeurIPS 2018) [paper](https://papers.nips.cc/paper/2018/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf) [code](https://github.com/OanaMariaCamburu/e-SNLI) 
5. **Measuring Association Between Labels and Free-Text Rationales** - 2021 - EMNLP 2021 [paper](https://arxiv.org/pdf/2010.12762.pdf) [code](https://github.com/allenai/label_rationale_association) 

#### [Unsupervised Selective Rationalization](#content)

1. **Rationalizing Neural Predictions** - 2016 - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D16-1011.pdf) [official code](https://github.com/taolei87/rcnn) 
2. **Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control** - 2019 - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [paper](https://arxiv.org/pdf/1910.13294.pdf)  [official code](https://github.com/Gorov/three_player_for_emnlp) 
3. **Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control** - 2019 - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [paper](https://arxiv.org/pdf/1910.13294.pdf)  [official code](https://github.com/Gorov/three_player_for_emnlp) 
4. **Learning to Faithfully Rationalize by Construction** - 2019 - Proceedings of the Association for Computational Linguistics (ACL) [paper](https://aclanthology.org/2020.acl-main.409.pdf)  [official code](https://github.com/successar/FRESH) 


#### [Attention Weights as Explanations](#content)
1. **Neural machine translation by jointly learning to align and translate** - 2015 - 3rd International Conference on Learning Representations, ICLR 2015 [paper](https://arxiv.org/abs/1409.0473)
2. **Attention is not Explanation** - 2019 - Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics [paper](https://aclanthology.org/N19-1357.pdf) [official code](https://github.com/successar/AttentionExplanation) 
3. **Is Attention Interpretable?** - 2019 - Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P19-1282.pdf) [official code](https://github.com/serrano-s/attn-tests) 
4. **Attention is not not Explanation** - 2019 - Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P19-1284v2.pdf) [official code](https://github.com/bastings/interpretable_predictions) 




#### [Anchors](#content)
1. **Anchors: High-Precision Model-Agnostic Explanations** - 2018 - AAAI Conference on Artificial Intelligence (AAAI) [paper](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf) [code](https://github.com/marcotcr/anchor) 


#### [Gradient-based explanations](#content)

1. **(Not NLP) On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation** - 2015 - International Conference on Machine Learning [paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140) 
2. **(Not NLP) Interpretation of Prediction Models Using the Input Gradient** - 2016 - 29th Conference on Neural Information Processing Systems (NIPS 2016) [paper](https://arxiv.org/pdf/1611.07634.pdf) 
3. **Visualizing and Understanding Neural Models in NLP** - 2016 - Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies [paper](https://aclanthology.org/N16-1082/) [official code](https://github.com/jiweil/Visualizing-and-Understanding-Neural-Models-in-NLP)
4. **(Not NLP) Smoothgrad: removing noise by adding noise** - 2017 - Workshop on Visualization for Deep Learning at ICML 2017 [paper](https://arxiv.org/abs/1706.03825) [code](https://github.com/pikahhh/pytorch-smoothgrad)
5. **(Not NLP) Learning Important Features Through Propagating Activation Differences** - 2017 - International Conference on Machine Learning [paper](https://dl.acm.org/doi/pdf/10.5555/3305890.3306006) [paper_v2](https://arxiv.org/pdf/1704.02685.pdf) [official code](https://github.com/kundajelab/deeplift)
6. **(Not NLP) Axiomatic attribution for deep networks** - 2017 - International Conference on Machine Learning [paper](https://dl.acm.org/doi/10.5555/3305890.3306024) [official code](https://github.com/ankurtaly/Integrated-Gradients) [Pytorch implementation](https://github.com/TianhongDai/integrated-gradient-pytorch)
7. **Explaining Recurrent Neural Network Predictions in Sentiment Analysis** - 2017 - Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis [paper](https://aclanthology.org/W17-5221.pdf) [official code](https://github.com/ArrasL/LRP_for_LSTM)
8. **" What is relevant in a text document?": An interpretable machine learning approach** - 2017 - PloS one [paper](https://arxiv.org/abs/1612.07843) [official code](https://github.com/ArrasL/LRP_for_LSTM)
9. **Did the Model Understand the Question?** - 2018 - Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P18-1176/) [official code](https://github.com/pramodkaushik/acl18_results)
10. **Incorporating Priors with Feature Attribution on Text Classification** - 2019 - Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P19-1631.pdf) [official code](https://github.com/frederick0329/Incorporating-Priors)
11. **Explaining and Interpreting LSTMs** - 2019 - Explainable ai: Interpreting, explaining and visualizing deep learning [paper](https://arxiv.org/pdf/1909.12114.pdf) [official code](https://github.com/ArrasL/LRP_for_LSTM)
12. **Gradient-based Analysis of NLP Models is Manipulable** - 2020 - Findings of the Association for Computational Linguistics: EMNLP 2020 [paper](https://aclanthology.org/2020.findings-emnlp.24.pdf) [code](https://github.com/ucinlp/facade/tree/facade) [site](https://ucinlp.github.io/facade/) 
13. **Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing.** - 2021 - Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP [paper](https://aclanthology.org/2021.blackboxnlp-1.33.pdf) [code](https://github.com/qdata/textattack-fragile-interpretations)

#### [Destillation](#content)
1. **(Not NLP) Distilling the knowledge in a neural network** - 2015 - arxiv [paper](https://arxiv.org/abs/1503.02531) [code](https://github.com/shriramsb/Distilling-the-Knowledge-in-a-Neural-Network)
2. **(Not NLP) Distilling a neural network into a soft decision tree** - 2017 - arxiv [paper](https://arxiv.org/pdf/1711.09784.pdf) [code](https://github.com/xuyxu/Soft-Decision-Tree)
3. **(Not NLP) Born-Again Neural Networks** - 2018 - Proceedings of the 35 th International Conference on Machine Learning [paper](https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf) [code](https://github.com/nocotan/born_again_neuralnet)
4. **(Not NLP) Distill-and-compare: Auditing black-box models using transparent model distillation** - 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society [paper](https://dl.acm.org/doi/abs/10.1145/3278721.3278725)
5. **(Not NLP) Improving the Interpretability of Deep Neural Networks with Knowledge Distillation** - 2018 - IEEE International Conference on Data Mining Workshops (ICDMW) [paper](https://arxiv.org/pdf/1812.10924.pdf)
6. **(Not NLP) Learning global additive explanations for neural nets using model distillation** - 2018 - arxiv [paper](https://arxiv.org/abs/1801.08640)

#### [Counterfactual explanations](#content)
1. **A causal framework for explaining the predictions of black-box sequence-to-sequence models.** - 2017 - Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D17-1042.pdf) 
2. **(Not NLP) On the computation of counterfactual explanations--A survey** - 2019 - Arxiv [paper](https://arxiv.org/pdf/1911.07749.pdf) [site](https://github.com/wangyongjie-ntu/awesome-counterfactual-explanations)
3. **Explaining classifiers with causal concept effect (cace)** - 2019 - Arxiv [paper](https://arxiv.org/abs/1907.07165)
4. **Investigating Gender Bias in Language Models Using Causal Mediation Analysis.** - 2020 - 34th Conference on Neural Information Processing Systems (NeurIPS 2020) [paper arxiv](https://arxiv.org/pdf/2004.12265.pdf) [paper neurips](https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf)  [code](https://github.com/sebastianGehrmann/CausalMediationAnalysis) 
5. **Reducing Sentiment Bias in Language Models via Counterfactual Evaluation.** - 2020 - Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings [paper](https://aclanthology.org/2020.findings-emnlp.7.pdf)
6. **Counterfactual Memorization in Neural Language Models** - 2021 - Arxiv [paper](https://arxiv.org/abs/2112.12938)
7. **CausaLM: Causal Model Explanation Through Counterfactual Language Models** - 2021 - Computational Linguistics (2021) [paper](https://direct.mit.edu/coli/article/47/2/333/98518/CausaLM-Causal-Model-Explanation-Through) [code](https://github.com/amirfeder/CausaLM) [data](https://www.kaggle.com/amirfeder/causalm) 
8. **Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals.** - 2021 - Transactions of the Association for Computational Linguistics [paper](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00359/98091)  [code](https://github.com/yanaiela/amnesic_probing) 
9. **Explaining nlp models via minimal contrastive editing (mice).** - 2021 - Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 [paper](https://aclanthology.org/2021.findings-acl.336.pdf)  [code](https://github.com/allenai/mice) 
10. **Polyjuice: Automated, general-purpose counterfactual generation.** - 2021 - ACL 2021 [paper](https://arxiv.org/pdf/2101.00288.pdf)  [code](https://github.com/tongshuangwu/polyjuice)  [site](https://huggingface.co/uw-hai/polyjuice) 




#### [Neurons' Activatons](#content)
1. **Visualizing and Understanding Recurrent Networks** - 2016 - Proceedings of ICLR Conference Track [paper](http://vision.stanford.edu/pdf/KarpathyICLR2016.pdf) [code](https://github.com/shriramsb/Distilling-the-Knowledge-in-a-Neural-Network)
2. **Learning to generate reviews and discovering sentiment** - 2017 - arxiv [paper](https://arxiv.org/abs/1704.01444) [official code](https://github.com/openai/generating-reviews-discovering-sentiment) [Pytorch implementation](https://github.com/guillitte/pytorch-sentiment-neuron)
3. **Representation of linguistic form and function in recurrent neural networks** - 2017 - Computational Linguistics [paper](https://direct.mit.edu/coli/article/43/4/761/1583/Representation-of-Linguistic-Form-and-Function-in) 
4. **Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging** - 2019 - Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics [paper](https://aclanthology.org/N19-1416.pdf) 

#### [Influence Functions](#content)
1. **(Not NLP) Understanding Black-box Predictions via Influence Functions** - 2017 - International Conference on Machine Learning [paper](https://proceedings.mlr.press/v70/koh17a/koh17a.pdf) [code](https://github.com/kohpangwei/influence-release)
2. **Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions** - 2020 - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/2020.acl-main.492.pdf) [code](https://github.com/xhan77/influence-function-analysis)
3. **(Not NLP) Influence functions in deep learning are fragile** - 2021 - International Conference on Learning Representations [paper](https://arxiv.org/pdf/2006.14651.pdf) [code](https://github.com/xhan77/influence-function-analysis)
