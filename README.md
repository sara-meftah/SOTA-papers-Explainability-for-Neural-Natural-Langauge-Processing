# Tracking SOTA papers about Explainability for Neural Natural Langauge Processing


#### [Surveys](#content)
1. **A Survey on Explainability in Machine Reading Comprehension.** - 2020 - Arxiv [paper](https://arxiv.org/abs/2010.00389)
2. **A Diagnostic Study of Explainability Techniques for Text Classification.** - 2020 - Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/2020.emnlp-main.263.pdf) 
3. **A Survey of the State of Explainable AI for Natural Language Processing.** - 2020 - AACL 2020 [paper](https://arxiv.org/abs/2010.00711) [site](https://xainlp2020.github.io/xainlp/)
4. **( Not NLP) Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics** - 2021 - MDPI [paper](https://www.mdpi.com/2079-9292/10/5/593)  



#### [Evaluation](#content)

1. **( Not NLP) On the Robustness of Interpretability Methods** - 2018 - Arxiv [paper](https://arxiv.org/abs/1806.08049)
4. **( Not NLP) On the (In)fidelity and Sensitivity of Explanations** - 2019 - 33rd Conference on Neural Information Processing Systems (NeurIPS 2019) [paper](https://papers.nips.cc/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf)  [code](https://github.com/chihkuanyeh/saliency_evaluation)
5. **Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?** - 2020 - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/2020.acl-main.491.pdf) [code](https://github.com/peterbhase/InterpretableNLP-ACL2020)
6. **Evaluating Explanation Methods for Neural Machine Translation** - 2020 - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/2020.acl-main.35.pdf)
7. **( Not NLP) Fooling lime and shap: Adversarial attacks on post hoc explanation methods** - 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society [paper](https://dl.acm.org/doi/pdf/10.1145/3375627.3375830) [official code](https://github.com/dylan-slack/Fooling-LIME-SHAP)
8. **( Not NLP) What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors** - 2021 - KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining [paper](https://dl.acm.org/doi/10.1145/3447548.3467213)  
9. **The Eval4NLP Shared Task on Explainable Quality Estimation: Overview and Results.** - 2021 - Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems [paper](https://aclanthology.org/2021.eval4nlp-1.17.pdf)
10. **Measuring and Improving Faithfulness of Attention in Neural Machine Translation** - 2021 - Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics [paper](https://aclanthology.org/2021.eacl-main.243.pdf)


#### [Datasets](#content)

1. **HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.** - 2018 - Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D18-1259/) [data](https://hotpotqa.github.io/)
2. **QUACKIE: A NLP Classification Task With Ground Truth Explanations.** - 2020 - Arxiv [paper](https://aclanthology.org/D18-1259/) [code](https://github.com/axa-rev-research/quackie)  [site](https://axa-rev-research.github.io/quackie/)




#### [Probing Classifiers](#content)

1. **Does string-based neural MT learn source syntax?** - 2016 - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D16-1159.pdf)
2. **Fine-grained analysis of sentence embeddings using auxiliary prediction tasks.** - 2016 - Proceedings of ICLR Conference Track [paper](https://arxiv.org/pdf/1608.04207.pdf)
3. **Assessing the ability of LSTMs to learn syntax-sensitive dependencies.** - 2016 - Transactions of the Association for Computational Linguistics [paper](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00115/43378/Assessing-the-Ability-of-LSTMs-to-Learn-Syntax) [code](https://github.com/TalLinzen/rnn_agreement)
4. **What you can cram into a single vector: Probing sentence embeddings for linguistic properties.** - 2018 - Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P18-1198.pdf) [data](https://github.com/facebookresearch/SentEval/tree/main/data/probing)
5. **Syntactic structure from deep learning.** - 2021 - Annual Review of Linguistics [paper](https://www.annualreviews.org/doi/abs/10.1146/annurev-linguistics-032020-051035) 
6. **Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?** - 2021 - Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics [paper](https://aclanthology.org/2021.eacl-main.295.pdf) 
7. **Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals.** - 2021 - Transactions of the Association for Computational Linguistics [paper](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00359/98091)  [code](https://github.com/yanaiela/amnesic_probing) 


#### [Surrogate models](#content)

1. **Why should i trust you?: Explaining the predictions of any classifier.** - 2016 - Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining [paper](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf) [code](https://github.com/marcotcr/lime) 
2. **A causal framework for explaining the predictions of black-box sequence-to-sequence models.** - 2017 - Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D17-1042.pdf) [code](https://github.com/marcotcr/lime) 
3. **( Not NLP) "Why Should You Trust My Explanation?" Understanding Uncertainty in LIME Explanations** - 2019 - Appearing at the International Conference on Machine Learning AI for Social Good Workshop [paper](https://arxiv.org/abs/1904.12991)
4.  **An analysis of lime for text data.** - 2021 - Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS) 2021 [paper](http://proceedings.mlr.press/v130/mardaoui21a/mardaoui21a.pdf) 
5. **Explaining the explainer: A first theoretical analysis of LIME.** - 2020 - Proceedings of the 23th International Conference on Artificial Intelligence and Statistics (AISTATS) 2020 [paper](http://proceedings.mlr.press/v108/garreau20a/garreau20a.pdf) 
6. **( Not NLP) Fooling lime and shap: Adversarial attacks on post hoc explanation methods** - 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society [paper](https://dl.acm.org/doi/pdf/10.1145/3375627.3375830) [official code](https://github.com/dylan-slack/Fooling-LIME-SHAP)
7. **Deterministic Local Interpretable Model-Agnostic Explanations for Stable Explainability.** - 2021 - Special Issue Advances in Explainable Artificial Intelligence (XAI) [paper](https://aclanthology.org/D18-1259/) [code](https://www.mdpi.com/2504-4990/3/3/27/htm)  [site](https://github.com/rehmanzafar/dlime_experiments)
8. **Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing.** - 2021 - Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP [paper](https://aclanthology.org/2021.blackboxnlp-1.33.pdf) [code](https://github.com/qdata/textattack-fragile-interpretations)

#### [Supervised Rationalization](#content)

1. **Using “annotator rationales” to improve machine learning for text categorization** - 2007 - Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics (NAACL 2007) [paper](https://aclanthology.org/N07-1033/) 
2. **Rationale-Augmented Convolutional Neural Networks for Text Classification** - 2016 - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D16-1076/)  [official code](https://github.com/bwallace/rationale-CNN)  [other implementation](https://github.com/yezhang-xiaofan/Rationale-CNN) 
3. **Training Classifiers with Natural Language Explanations** - 2018 - Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P18-1175.pdf) 
4. **e-SNLI: Natural Language Inference with Natural Language Explanations** - 2018 - 32nd Conference on Neural Information Processing Systems (NeurIPS 2018) [paper](https://papers.nips.cc/paper/2018/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf) [code](https://github.com/OanaMariaCamburu/e-SNLI) 
5. **Measuring Association Between Labels and Free-Text Rationales** - 2021 - EMNLP 2021 [paper](https://arxiv.org/pdf/2010.12762.pdf) [code](https://github.com/allenai/label_rationale_association) 

#### [Unsupervised Rationalization](#content)

1. **Rationalizing Neural Predictions** - 2016 - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [paper](https://aclanthology.org/D16-1011.pdf) [official code](https://github.com/taolei87/rcnn) 
2. **Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control** - 2019 - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing [paper](https://arxiv.org/pdf/1910.13294.pdf)  [official code](https://github.com/Gorov/three_player_for_emnlp) 
3. **Learning to Faithfully Rationalize by Construction** - 2020 - Proceedings of the Association for Computational Linguistics (ACL) [paper](https://aclanthology.org/2020.acl-main.409.pdf)  [official code](https://github.com/successar/FRESH) 


#### [Attention Weights as Explanations](#content)
1. **Neural machine translation by jointly learning to align and translate** - 2015 - 3rd International Conference on Learning Representations, ICLR 2015 [paper](https://arxiv.org/abs/1409.0473)
2. **Attention is not Explanation** - 2019 - Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics [paper](https://aclanthology.org/N19-1357.pdf) [official code](https://github.com/successar/AttentionExplanation) 
3. **Is Attention Interpretable?** - 2019 - Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P19-1282.pdf) [official code](https://github.com/serrano-s/attn-tests) 
4. **Attention is not not Explanation** - 2019 - proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) [paper](https://aclanthology.org/D19-1002.pdf) [official code](https://github.com/sarahwie/attention) 




#### [Anchors](#content)
1. **Anchors: High-Precision Model-Agnostic Explanations** - 2018 - AAAI Conference on Artificial Intelligence (AAAI) [paper](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf) [code](https://github.com/marcotcr/anchor) 


#### [Gradient-based explanations](#content)

1. **(Not NLP) On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation** - 2015 - International Conference on Machine Learning [paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140) 
2. **Visualizing and Understanding Neural Models in NLP** - 2016 - Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies [paper](https://aclanthology.org/N16-1082/) [official code](https://github.com/jiweil/Visualizing-and-Understanding-Neural-Models-in-NLP)
3. **(Not NLP) Smoothgrad: removing noise by adding noise** - 2017 - Workshop on Visualization for Deep Learning at ICML 2017 [paper](https://arxiv.org/abs/1706.03825) [code](https://github.com/pikahhh/pytorch-smoothgrad)
4. **(Not NLP) Learning Important Features Through Propagating Activation Differences** - 2017 - International Conference on Machine Learning [paper](https://dl.acm.org/doi/pdf/10.5555/3305890.3306006) [paper_v2](https://arxiv.org/pdf/1704.02685.pdf) [official code](https://github.com/kundajelab/deeplift)
5. **(Not NLP) Axiomatic attribution for deep networks** - 2017 - International Conference on Machine Learning [paper](https://dl.acm.org/doi/10.5555/3305890.3306024) [official code](https://github.com/ankurtaly/Integrated-Gradients) [Pytorch implementation](https://github.com/TianhongDai/integrated-gradient-pytorch)
6. **Explaining Recurrent Neural Network Predictions in Sentiment Analysis** - 2017 - Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis [paper](https://aclanthology.org/W17-5221.pdf) [official code](https://github.com/ArrasL/LRP_for_LSTM)
7. **" What is relevant in a text document?": An interpretable machine learning approach** - 2017 - PloS one [paper](https://arxiv.org/abs/1612.07843) [official code](https://github.com/ArrasL/LRP_for_LSTM)
8. **Did the Model Understand the Question?** - 2018 - Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P18-1176/) [official code](https://github.com/pramodkaushik/acl18_results)
9. **Incorporating Priors with Feature Attribution on Text Classification** - 2019 - Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics [paper](https://aclanthology.org/P19-1631.pdf) [official code](https://github.com/frederick0329/Incorporating-Priors)
10. **Gradient-based Analysis of NLP Models is Manipulable** - 2020 - Findings of the Association for Computational Linguistics: EMNLP 2020 [paper](https://aclanthology.org/2020.findings-emnlp.24.pdf) [code](https://github.com/ucinlp/facade/tree/facade) [site](https://ucinlp.github.io/facade/) 
11. **Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing.** - 2021 - Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP [paper](https://aclanthology.org/2021.blackboxnlp-1.33.pdf) [code](https://github.com/qdata/textattack-fragile-interpretations)

#### [Destillation](#content)
1. **(Not NLP) Distilling the knowledge in a neural network** - 2015 - arxiv [paper](https://arxiv.org/abs/1503.02531) [code](https://github.com/shriramsb/Distilling-the-Knowledge-in-a-Neural-Network)
2. **(Not NLP) Distilling a neural network into a soft decision tree** - 2017 - arxiv [paper](https://arxiv.org/pdf/1711.09784.pdf) [code](https://github.com/xuyxu/Soft-Decision-Tree)
3. **(Not NLP) Born-Again Neural Networks** - 2018 - Proceedings of the 35 th International Conference on Machine Learning [paper](https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf) [code](https://github.com/nocotan/born_again_neuralnet)
4. **(Not NLP) Distill-and-compare: Auditing black-box models using transparent model distillation** - 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society [paper](https://dl.acm.org/doi/abs/10.1145/3278721.3278725)
5. **(Not NLP) Improving the Interpretability of Deep Neural Networks with Knowledge Distillation** - 2018 - IEEE International Conference on Data Mining Workshops (ICDMW) [paper](https://arxiv.org/pdf/1812.10924.pdf)
6. **(Not NLP) Learning global additive explanations for neural nets using model distillation** - 2018 - arxiv [paper](https://arxiv.org/abs/1801.08640)

#### [Counterfactual explanations](#content)
1. **Counterfactual Memorization in Neural Language Models** - 2021 - Arxiv [paper](https://arxiv.org/abs/2112.12938)
2. **CausaLM: Causal Model Explanation Through Counterfactual Language Models** - 2021 - Computational Linguistics (2021) [paper](https://direct.mit.edu/coli/article/47/2/333/98518/CausaLM-Causal-Model-Explanation-Through) [code](https://github.com/amirfeder/CausaLM) [data](https://www.kaggle.com/amirfeder/causalm) 





